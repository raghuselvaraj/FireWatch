"""Kafka Streams application for real-time forest fire detection using ML models."""
import json
import base64
import numpy as np
import cv2
import os
import sys
import signal
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import KafkaError
from ultralytics import YOLO

# Suppress torchvision deprecation warnings (from fire-detect-nn library)
warnings.filterwarnings('ignore', category=UserWarning, module='torchvision')
# Suppress x265 codec warnings (not actionable)
warnings.filterwarnings('ignore', message='.*x265.*')

# Add parent directory to path to import config and modules
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import config

# Import fire-detect-nn if using that model type
# Try importing from installed package first, then fall back to local directory
FIRE_DETECT_NN_AVAILABLE = False
if config.ML_MODEL_TYPE == "fire-detect-nn" or config.ML_MODEL_SOURCE == "fire-detect-nn":
    try:
        # First try importing from installed package (site-packages)
        try:
            import fire_detect_nn
            FIRE_DETECT_NN_AVAILABLE = True
            print("‚úì fire-detect-nn found in site-packages")
        except ImportError:
            # Fall back to local directory (for backwards compatibility)
            fire_detect_dir = project_root / config.FIRE_DETECT_NN_DIR
            if fire_detect_dir.exists():
                sys.path.insert(0, str(fire_detect_dir))
                FIRE_DETECT_NN_AVAILABLE = True
                print(f"‚úì fire-detect-nn found in local directory: {fire_detect_dir}")
            else:
                print(f"‚ö†Ô∏è  fire-detect-nn not found in site-packages or local directory")
                print(f"   Run: python3 scripts/install_fire_detect_nn.py")
    except Exception as e:
        print(f"‚ö†Ô∏è  fire-detect-nn not available: {e}")


class FireDetectionModel:
    """Unified fire detection model supporting fire-detect-nn (default) and Ultralytics YOLOv8 (alternative)."""
    
    def __init__(self, model_path: str = None):
        """Initialize the fire detection model."""
        self.model_path = model_path or config.ML_MODEL_PATH
        self.confidence_threshold = config.CONFIDENCE_THRESHOLD
        self.iou_threshold = config.IOU_THRESHOLD
        self.model_source = config.ML_MODEL_SOURCE
        self.model_name = config.ML_MODEL_NAME
        self.model_type = config.ML_MODEL_TYPE
        
        # Determine which model to use
        self.use_fire_detect_nn = (self.model_type == "fire-detect-nn" or self.model_source == "fire-detect-nn")
        
        if self.use_fire_detect_nn:
            # Load fire-detect-nn model
            self.model, self.device = self._load_fire_detect_nn()
            print(f"fire-detect-nn model initialized (DenseNet121)")
        else:
            # Load Ultralytics YOLOv8 model
            self.model = self._load_model()
            print(f"Fire detection model initialized: {self.model_name}")
        
        print(f"Confidence threshold: {self.confidence_threshold}, IOU threshold: {self.iou_threshold}")
    
    def _load_fire_detect_nn(self):
        """Load fire-detect-nn model by importing from installed package or local directory."""
        try:
            import torch
            import site
            
            # Try to import from installed package first
            try:
                import fire_detect_nn
                # Get the installed package path
                fire_detect_nn_path = Path(fire_detect_nn.__file__).parent
                from fire_detect_nn.models import FireClassifier
                from fire_detect_nn.datasets.combo import transform as fire_transform
                weights_path = fire_detect_nn_path / "weights" / "firedetect-densenet121-pretrained.pt"
                print(f"Using fire-detect-nn from installed package: {fire_detect_nn_path}")
            except ImportError:
                # Fall back to local directory (for backwards compatibility)
                fire_detect_dir = project_root / config.FIRE_DETECT_NN_DIR
                if not fire_detect_dir.exists():
                    raise FileNotFoundError(
                        f"fire-detect-nn not found in site-packages or local directory.\n"
                        f"Run: python3 scripts/install_fire_detect_nn.py"
                    )
                sys.path.insert(0, str(fire_detect_dir))
                from models import FireClassifier
                from datasets.combo import transform as fire_transform
                weights_path = project_root / config.FIRE_DETECT_NN_WEIGHTS
                print(f"Using fire-detect-nn from local directory: {fire_detect_dir}")
            
            if not weights_path.exists():
                raise FileNotFoundError(
                    f"fire-detect-nn weights not found: {weights_path}\n"
                    f"Run: python3 scripts/install_fire_detect_nn.py to download weights"
                )
            
            print(f"Loading fire-detect-nn model from: {weights_path}")
            
            # Use FireClassifier from the repository (DenseNet121 with binary classification)
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = FireClassifier(backbone='densenet121', pretrained=False)
            
            # Load pretrained weights
            state_dict = torch.load(weights_path, map_location=device)
            model.load_state_dict(state_dict)
            model.eval()
            model.to(device)
            
            # Store transform for preprocessing
            self.fire_transform = fire_transform
            
            # Enable gradient computation for GradCAM (even though model is in eval mode)
            # We'll compute gradients manually when needed
            
            print(f"‚úì fire-detect-nn model loaded successfully on {device}")
            return model, device
            
        except ImportError as e:
            raise ImportError(
                f"Required modules not found. Make sure fire-detect-nn is installed.\n"
                f"Run: python3 scripts/install_fire_detect_nn.py\n"
                f"Original error: {e}"
            )
        except Exception as e:
            raise RuntimeError(f"Failed to load fire-detect-nn model: {e}")
    
    def _load_model(self) -> YOLO:
        """Load YOLO model from local path or Hugging Face."""
        try:
            if self.model_source == "local" and os.path.exists(self.model_path):
                # Load from local file
                print(f"Loading model from local path: {self.model_path}")
                model = YOLO(self.model_path)
                self._print_model_info(model)
                return model
            elif self.model_source == "huggingface":
                # Try to load from Hugging Face Hub
                print(f"Attempting to load from Hugging Face: {self.model_name}")
                try:
                    model = YOLO(self.model_name)
                    self._print_model_info(model)
                    # Try to save locally for future use
                    try:
                        os.makedirs(os.path.dirname(self.model_path) if os.path.dirname(self.model_path) else "models", exist_ok=True)
                        model.save(self.model_path)
                        print(f"Model saved to: {self.model_path}")
                    except:
                        pass
                    return model
                except Exception as hf_error:
                    print(f"‚ö†Ô∏è  Could not load from Hugging Face: {hf_error}")
                    print(f"   Model '{self.model_name}' may not exist or be accessible")
                    # Fall through to try local file or default
            else:
                # Try to download from Hugging Face and save locally
                print(f"Attempting to download from Hugging Face: {self.model_name}")
                try:
                    model = YOLO(self.model_name)
                    self._print_model_info(model)
                    os.makedirs(os.path.dirname(self.model_path) if os.path.dirname(self.model_path) else "models", exist_ok=True)
                    model.save(self.model_path)
                    print(f"Model saved to: {self.model_path}")
                    return model
                except Exception as dl_error:
                    print(f"‚ö†Ô∏è  Could not download model: {dl_error}")
            
            # Try local file as fallback
            if os.path.exists(self.model_path):
                print(f"Using local model file: {self.model_path}")
                model = YOLO(self.model_path)
                self._print_model_info(model)
                return model
            
            # Final fallback - raise error instead of using non-fire-detection model
            raise ValueError(
                f"Could not load YOLOv8 model from any source.\n"
                f"  Tried: {self.model_path}, {self.model_name}\n"
                f"  To fix:\n"
                f"  1. Set ML_MODEL_TYPE=fire-detect-nn (recommended, default)\n"
                f"  2. Or provide a valid fire detection YOLOv8 model\n"
                f"  3. Set ML_MODEL_SOURCE=local and ML_MODEL_PATH to your model file"
            )
            
        except Exception as e:
            raise ValueError(
                f"Error loading YOLOv8 model: {e}\n"
                f"  To fix:\n"
                f"  1. Set ML_MODEL_TYPE=fire-detect-nn (recommended, default)\n"
                f"  2. Or provide a valid fire detection YOLOv8 model\n"
                f"  3. Set ML_MODEL_SOURCE=local and ML_MODEL_PATH to your model file"
            ) from e
    
    def _print_model_info(self, model: YOLO):
        """Print information about the loaded model."""
        try:
            if hasattr(model, 'names') and model.names:
                classes = list(model.names.values())
                fire_classes = [c for c in classes if any(kw in c.lower() for kw in ['fire', 'smoke', 'flame', 'burn'])]
                print(f"   Model classes: {len(classes)} total")
                if fire_classes:
                    print(f"   ‚úì Fire-related classes found: {fire_classes}")
                else:
                    print(f"   ‚ö†Ô∏è  No fire-related classes found in model!")
                    print(f"   ‚ö†Ô∏è  This model may not be suitable for fire detection")
                    print(f"   ‚ö†Ô∏è  First few classes: {classes[:10]}")
        except:
            pass
    
    def predict(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        Predict if frame contains fire using the configured model.
        
        Args:
            frame: Input frame as numpy array (BGR format from OpenCV)
        
        Returns:
            Dictionary with prediction results including bounding boxes
        """
        # Use fire-detect-nn if configured
        if self.use_fire_detect_nn:
            return self._predict_fire_detect_nn(frame)
        
        # Otherwise use Ultralytics YOLOv8
        try:
            # YOLO expects RGB format, so convert from BGR
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # Run inference
            # Use lower confidence for model.predict to see all detections
            # We'll filter by fire classes and threshold later
            inference_conf = min(0.1, self.confidence_threshold)  # Lower threshold to see more
            results = self.model.predict(
                frame_rgb,
                conf=inference_conf,
                iou=self.iou_threshold,
                verbose=False,
                imgsz=640  # YOLOv8 standard input size
            )
            
            # Process results
            detections = []
            max_confidence = 0.0
            has_fire = False
            all_detections_debug = []  # For debugging
            
            for result in results:
                boxes = result.boxes
                
                if boxes is not None and len(boxes) > 0:
                    for box in boxes:
                        # Get box coordinates (xyxy format)
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        confidence = box.conf[0].cpu().item()  # Use .item() to get Python scalar (avoids NumPy deprecation)
                        class_id = int(box.cls[0].cpu().item())  # Use .item() to get Python scalar
                        class_name = result.names[class_id] if hasattr(result, 'names') else f"class_{class_id}"
                        
                        # Store all detections for debugging (first 10 frames only)
                        all_detections_debug.append((class_name, confidence))
                        
                        # Check if this is a fire-related detection
                        # Common fire-related class names: fire, smoke, flame, etc.
                        # Exclude false positives like "fire hydrant", "fire truck", etc.
                        fire_keywords = ['fire', 'smoke', 'flame', 'burn', 'wildfire']
                        exclude_keywords = ['hydrant', 'truck', 'extinguisher', 'alarm', 'station', 'engine']
                        
                        class_lower = class_name.lower()
                        is_fire_class = any(keyword in class_lower for keyword in fire_keywords)
                        is_excluded = any(exclude in class_lower for exclude in exclude_keywords)
                        
                        # Only consider it fire if:
                        # 1. The class name contains fire keywords AND
                        # 2. It's not an excluded class (like "fire hydrant") AND
                        # 3. The confidence is above threshold
                        if is_fire_class and not is_excluded and confidence > self.confidence_threshold:
                            has_fire = True
                            max_confidence = max(max_confidence, confidence)
                            
                            detections.append({
                                "bbox": [int(x1), int(y1), int(x2), int(y2)],
                                "confidence": float(confidence),
                                "class": class_name,
                                "class_id": class_id
                            })
            
            # Debug output (only for first few frames to avoid spam)
            import os
            debug_file = os.getenv("DEBUG_DETECTIONS", "")
            if debug_file or (hasattr(self, '_debug_frame_count') and self._debug_frame_count < 5):
                if not hasattr(self, '_debug_frame_count'):
                    self._debug_frame_count = 0
                if self._debug_frame_count < 5:
                    if all_detections_debug:
                        print(f"   [DEBUG] Frame detections: {[(c, f'{conf:.1%}') for c, conf in all_detections_debug[:5]]}")
                    else:
                        print(f"   [DEBUG] No detections in frame (threshold: {inference_conf})")
                self._debug_frame_count += 1
            
            return {
                "has_fire": bool(has_fire),
                "fire_probability": float(max_confidence) if has_fire else 0.0,
                "detections": detections,
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            print(f"Error during prediction: {e}")
            # Return safe default on error
            return {
                "has_fire": False,
                "fire_probability": 0.0,
                "detections": [],
                "timestamp": datetime.utcnow().isoformat(),
                "error": str(e)
            }
    
    def _compute_gradcam_heatmap(self, frame: np.ndarray) -> Optional[np.ndarray]:
        """
        Compute GradCAM heatmap for fire region visualization.
        
        Args:
            frame: Input frame as numpy array (BGR format from OpenCV)
        
        Returns:
            2D numpy array heatmap, or None if computation fails
        """
        try:
            import torch
            from PIL import Image
            
            # Convert BGR to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # Convert to PIL Image and apply fire-detect-nn transform
            pil_image = Image.fromarray(frame_rgb)
            input_tensor = self.fire_transform(pil_image).unsqueeze(0).to(self.device)
            input_tensor.requires_grad = True
            
            # Get activations from the last feature layer before classifier
            # For DenseNet121, this is the features output
            activations = None
            gradients = None
            
            def forward_hook(module, input, output):
                nonlocal activations
                activations = output
            
            def backward_hook(module, grad_input, grad_output):
                nonlocal gradients
                if grad_output[0] is not None:
                    gradients = grad_output[0]
            
            # Register hooks on the features layer (before global pooling)
            # DenseNet121 backbone has a 'features' attribute
            hook_handle = None
            grad_handle = None
            
            if hasattr(self.model.backbone, 'features'):
                # Hook into the features output (before global pooling)
                features_layer = self.model.backbone.features
                hook_handle = features_layer.register_forward_hook(forward_hook)
                grad_handle = features_layer.register_backward_hook(backward_hook)
            
            # Forward pass (need to enable gradients)
            self.model.train()  # Temporarily enable training mode for gradients
            output = self.model(input_tensor)
            fire_score = output[0]
            
            # Backward pass
            self.model.zero_grad()
            fire_score.backward(retain_graph=False)
            self.model.eval()  # Back to eval mode
            
            # Compute GradCAM
            if activations is not None and gradients is not None:
                # activations shape: [batch, channels, height, width]
                # gradients shape: [batch, channels, height, width]
                
                # Pool gradients across spatial dimensions
                pooled_gradients = torch.mean(gradients, dim=[0, 2, 3], keepdim=True)
                
                # Weight activations by gradients
                weighted_activations = activations * pooled_gradients
                
                # Average over channels
                heatmap = torch.mean(weighted_activations, dim=1, keepdim=False)
                heatmap = heatmap[0].cpu().detach().numpy()  # Remove batch dimension
                
                # Apply ReLU
                heatmap = np.maximum(heatmap, 0)
                
                # Normalize
                if heatmap.max() > 0:
                    heatmap = heatmap / heatmap.max()
                
                # Remove hooks
                if hook_handle is not None:
                    hook_handle.remove()
                if grad_handle is not None:
                    grad_handle.remove()
                
                return heatmap
            
            # Clean up hooks if computation failed
            if hook_handle is not None:
                hook_handle.remove()
            if grad_handle is not None:
                grad_handle.remove()
            
            return None
            
        except Exception as e:
            # Silently fail - heatmap is optional
            # Reset model to eval mode
            try:
                self.model.eval()
            except:
                pass
            return None
    
    def _predict_fire_detect_nn(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        Predict using fire-detect-nn model (imported from GitHub repository).
        
        This is a binary classification model (fire/no-fire), so we return
        a full-frame detection if fire probability exceeds threshold.
        Also computes GradCAM heatmap for visual fire identification.
        
        Args:
            frame: Input frame as numpy array (BGR format from OpenCV)
        
        Returns:
            Dictionary with prediction results including heatmap
        """
        try:
            import torch
            from PIL import Image
            
            # Convert BGR to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # Convert to PIL Image and apply fire-detect-nn transform
            # (uses custom normalization: mean=(0.4005, 0.3702, 0.3419), std=(0.2858, 0.2749, 0.2742))
            pil_image = Image.fromarray(frame_rgb)
            input_tensor = self.fire_transform(pil_image).unsqueeze(0).to(self.device)
            
            # Run inference
            # FireClassifier outputs a single value (0-1) after sigmoid, where higher = more fire
            with torch.no_grad():
                output = self.model(input_tensor)
                # Extract single value from tensor to avoid NumPy deprecation warning
                fire_prob = output[0].cpu().item()  # Use .item() to get Python scalar
                no_fire_prob = 1.0 - fire_prob
            
            # Check if fire probability exceeds threshold
            has_fire = fire_prob >= self.confidence_threshold
            
            # Compute heatmap if fire detected (core feature - always enabled)
            heatmap = None
            if has_fire:
                try:
                    heatmap = self._compute_gradcam_heatmap(frame)
                except Exception as e:
                    # If GradCAM fails, continue without heatmap for this frame
                    # (but still process the frame)
                    pass
            
            # For classification models, we create a full-frame detection
            # since there are no bounding boxes
            detections = []
            if has_fire:
                h, w = frame.shape[:2]
                detections.append({
                    "bbox": [0, 0, w, h],  # Full frame
                    "confidence": fire_prob,
                    "class": "fire",
                    "class_id": 1
                })
            
            return {
                "has_fire": bool(has_fire),
                "fire_probability": float(fire_prob) if has_fire else 0.0,
                "detections": detections,
                "timestamp": datetime.utcnow().isoformat(),
                "model_type": "fire-detect-nn",
                "no_fire_probability": float(no_fire_prob),
                "heatmap": heatmap  # GradCAM heatmap for visual fire identification
            }
        
        except Exception as e:
            print(f"Error in fire-detect-nn prediction: {e}")
            import traceback
            traceback.print_exc()
            return {
                "has_fire": False,
                "fire_probability": 0.0,
                "detections": [],
                "timestamp": datetime.utcnow().isoformat(),
                "model_type": "fire-detect-nn",
                "error": str(e)
            }


def convert_numpy_types(obj):
    """Recursively convert numpy types to native Python types for JSON serialization."""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj


class FireDetectionStream:
    """Kafka Streams application for fire detection."""
    
    def __init__(self):
        """Initialize the stream processor."""
        # Use KAFKA_GROUP_ID from environment if set (for test runs with unique group IDs)
        # Otherwise use default group_id
        import os
        group_id = os.environ.get('KAFKA_GROUP_ID', 'fire-detection-stream')
        
        self.consumer = KafkaConsumer(
            config.KAFKA_VIDEO_TOPIC,
            bootstrap_servers=config.KAFKA_BOOTSTRAP_SERVERS,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            key_deserializer=lambda k: k.decode('utf-8') if k else None,
            group_id=group_id,  # Multiple consumers with same group_id share partitions
            auto_offset_reset='earliest',  # Read from beginning if no offset exists
            enable_auto_commit=False,  # Manual commit to ensure we process all messages
            consumer_timeout_ms=2147483647,  # Wait indefinitely for messages (max int32 value, ~24 days)
            max_poll_records=100,  # Process up to 100 records per poll for better throughput
            fetch_min_bytes=16384,  # Wait for at least 16KB before returning (increased for better throughput)
            fetch_max_wait_ms=100,  # Reduced wait time for lower latency
            max_partition_fetch_bytes=10485760  # 10MB per partition (increased for better throughput)
        )
        
        self.producer = KafkaProducer(
            bootstrap_servers=config.KAFKA_BOOTSTRAP_SERVERS,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda k: k.encode('utf-8') if k else None,
            acks=1,  # Only wait for leader acknowledgment (faster than 'all')
            retries=3,
            max_in_flight_requests_per_connection=5,  # Allow more in-flight for better throughput
            compression_type='gzip',  # Compress messages for better throughput
            batch_size=16384,  # Batch messages for better throughput
            linger_ms=10  # Wait up to 10ms to batch messages
        )
        
        self.model = FireDetectionModel()
        self.detections_topic = config.KAFKA_DETECTIONS_TOPIC
        self.video_completions_topic = config.KAFKA_VIDEO_COMPLETIONS_TOPIC
        
        # Track video state for incremental writing
        self.current_video_id = None
        self.video_writer = None  # OpenCV VideoWriter for current video
        self.video_fps = None
        self.video_width = None
        self.video_height = None
        self.video_filepath = None
        self.video_frame_count = 0
        self.video_start_timestamp = None  # First frame timestamp for filename
        self.last_frame_number = None  # Track last frame number to detect video end
        self.last_frame = None  # Store last frame for final buffer flush
        self.video_stats = {}  # Track stats per video: {video_id: {"frames": count, "fires": count, "max_prob": float}}
    
    def decode_frame(self, frame_data: str) -> np.ndarray:
        """Decode base64 frame data to numpy array."""
        frame_bytes = base64.b64decode(frame_data)
        nparr = np.frombuffer(frame_bytes, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        return frame
    
    def _initialize_video_writer(self, video_id: str, width: int, height: int, fps: float) -> Optional[str]:
        """Initialize video writer for a new video."""
        # Don't re-initialize if writer already exists
        if self.video_writer is not None:
            return self.video_filepath
            
        try:
            # Generate output path using the first frame timestamp
            timestamp_str = self.video_start_timestamp.replace(":", "-").replace(".", "-") if self.video_start_timestamp else datetime.utcnow().isoformat().replace(":", "-").replace(".", "-")
            output_dir = config.CLIP_STORAGE_PATH
            os.makedirs(output_dir, exist_ok=True)
            filename = f"{video_id}_with_heatmaps.mp4"
            filepath = os.path.join(output_dir, filename)
            
            # Create video writer
            # Try HEVC (H.265) first, fallback to H.264 (AVC), then mp4v
            fourcc = None
            codec_used = None
            
            for codec_name in ['HEVC', 'hvc1', 'avc1', 'H264', 'mp4v']:
                try:
                    test_fourcc = cv2.VideoWriter_fourcc(*codec_name)
                    # Test if codec works by creating a temporary writer
                    test_path = '/tmp/test_codec_firewatch.mp4'
                    test_writer = cv2.VideoWriter(test_path, test_fourcc, fps, (width, height))
                    if test_writer.isOpened():
                        test_writer.release()
                        # Clean up test file
                        if os.path.exists(test_path):
                            os.remove(test_path)
                        fourcc = test_fourcc
                        codec_used = codec_name
                        print(f"‚úì Using codec: {codec_name}")
                        break
                except Exception as e:
                    continue
            
            if fourcc is None:
                # Final fallback
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                codec_used = 'mp4v'
                print(f"‚ö†Ô∏è  Using fallback codec: mp4v")
            
            # Create VideoWriter
            # Note: OpenCV VideoWriter doesn't support quality/bitrate settings directly
            # File size depends on codec compression efficiency:
            # - HEVC/H.264: Better compression (smaller files)
            # - mp4v: Poor compression (larger files)
            # Resolution and frame rate also affect file size significantly
            self.video_writer = cv2.VideoWriter(filepath, fourcc, fps, (width, height))
            
            if not self.video_writer.isOpened():
                print(f"Error: Could not open video writer for {filepath}")
                self.video_writer = None
                return None
            
            # Log estimated file size info
            # Rough estimate: ~1-5 MB per minute at 640x480 depending on codec
            estimated_size_mb = (fps * 60 * 2.5) / 1024  # ~2.5MB per minute estimate
            print(f"  Codec: {codec_used}, Resolution: {width}x{height}, FPS: {fps:.1f}")
            print(f"  Estimated file size: ~{estimated_size_mb:.1f}MB per minute of video")
            
            self.video_filepath = filepath
            self.video_frame_count = 0
            print(f"üìπ Started writing video: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"Error initializing video writer: {e}")
            import traceback
            traceback.print_exc()
            self.video_writer = None
            return None
    
    def _publish_video_completion(self, video_id: str, local_filepath: str, stats: Dict[str, Any], video_metadata: Dict[str, Any]):
        """Publish video completion event to Kafka for S3 upload consumer."""
        try:
            completion_event = {
                "video_id": video_id,
                "local_filepath": local_filepath,
                "timestamp": datetime.utcnow().isoformat(),
                "stats": stats,
                "video_metadata": video_metadata
            }
            
            future = self.producer.send(
                self.video_completions_topic,
                key=video_id,
                value=completion_event
            )
            future.get(timeout=10)
            print(f"üì§ Published video completion event for {video_id}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error publishing video completion: {e}")
    
    def _close_video_writer(self, print_summary: bool = True) -> Optional[str]:
        """Close the current video writer and publish completion event for S3 upload."""
        if self.video_writer is not None:
            try:
                filepath = self.video_filepath
                frame_count = self.video_frame_count
                video_id = self.current_video_id
                
                # Properly release the video writer (this finalizes the file and writes moov atom)
                # This is critical - without proper release, the moov atom won't be written
                # and the file will be unplayable (moov atom not found error)
                #
                # IMPORTANT: OpenCV VideoWriter buffers frames in memory and only writes them
                # to disk when release() is called. If the process is killed before release(),
                # buffered frames are lost and the file size gets "stuck" at a partial size.
                # That's why the video was stuck at 23MB - frames were buffered but never written.
                file_size_before = os.path.getsize(filepath) if filepath and os.path.exists(filepath) else 0
                print(f"  Releasing video writer (frames written: {frame_count}, current file size: {file_size_before / 1024 / 1024:.1f}MB)...")
                
                # Ensure all frames are written before release
                # OpenCV VideoWriter buffers frames, so we need to make sure everything is flushed
                if self.video_writer is not None and self.video_writer.isOpened():
                    # Force buffer flush by writing a final frame (some codecs need this)
                    try:
                        # Get the last frame if available to ensure buffer is flushed
                        if self.last_frame is not None:
                            self.video_writer.write(self.last_frame)
                    except:
                        pass
                
                self.video_writer.release()
                self.video_writer = None  # Clear reference immediately
                
                # Force Python to flush file handles and garbage collect
                import gc
                gc.collect()
                
                # Give the OS time to flush file buffers and finalize the MP4 structure
                # The moov atom (metadata) is written during release, but we need to ensure
                # it's fully written to disk before proceeding
                import time
                time.sleep(3.0)  # Increased wait time to ensure proper finalization
                
                # Force file system sync to ensure data is written to disk
                try:
                    fd = os.open(filepath, os.O_RDONLY)
                    os.fsync(fd)  # Force sync to disk
                    os.close(fd)
                except:
                    pass
                
                # Check file size after release (should be larger as buffered frames are written)
                if filepath and os.path.exists(filepath):
                    file_size_after = os.path.getsize(filepath)
                    if file_size_after > file_size_before:
                        print(f"  ‚úì Video finalized: {file_size_after / 1024 / 1024:.1f}MB (wrote {file_size_after - file_size_before} bytes from buffer)")
                    else:
                        print(f"  ‚ö†Ô∏è  File size unchanged after release (may indicate issue)")
                
                # Verify the file exists and has content
                if filepath and os.path.exists(filepath):
                    file_size = os.path.getsize(filepath)
                    if file_size == 0:
                        print(f"‚ö†Ô∏è  Warning: Video file is empty: {filepath}")
                        return None
                    
                    # Verify video can be opened (checks for moov atom presence)
                    # Give extra time for file system to sync
                    import time
                    time.sleep(1.0)
                    
                    test_cap = cv2.VideoCapture(filepath)
                    if not test_cap.isOpened():
                        print(f"‚ö†Ô∏è  Warning: Video file may be corrupted (moov atom missing): {filepath}")
                        print(f"   This can happen if the process was terminated before video finalization")
                        test_cap.release()
                        return None
                    
                    # Check if video has valid frame count and duration
                    frame_count = int(test_cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    fps = test_cap.get(cv2.CAP_PROP_FPS)
                    test_cap.release()
                    
                    if frame_count == 0 or fps == 0:
                        print(f"‚ö†Ô∏è  Warning: Video file has 0 frames or invalid FPS: {filepath}")
                        print(f"   Expected {self.video_frame_count} frames at {self.video_fps} fps")
                        print(f"   This may indicate the video writer did not finalize properly")
                        # Don't return None - let the file exist even if it's not perfect
                        # The file might still be playable in some players
                
                # Get stats
                stats = self.video_stats.get(video_id, {})
                fire_count = stats.get("fires", 0)
                total_frames = stats.get("frames", frame_count)
                max_prob = stats.get("max_prob", 0.0)
                
                # Publish video completion event for S3 upload consumer
                if video_id and filepath and os.path.exists(filepath):
                    self._publish_video_completion(
                        video_id,
                        filepath,
                        {
                            "total_frames": total_frames,
                            "fire_count": fire_count,
                            "max_fire_probability": max_prob
                        },
                        {
                            "fps": self.video_fps,
                            "width": self.video_width,
                            "height": self.video_height,
                            "frame_count": frame_count
                        }
                    )
                
                # Print summary if requested
                if print_summary and video_id:
                    print(f"\n{'='*60}")
                    print(f"üìπ Video Complete: {video_id}")
                    print(f"{'='*60}")
                    print(f"  Local file: {filepath}")
                    print(f"  Total frames: {total_frames}")
                    print(f"  Frames with fire: {fire_count}")
                    print(f"  Max fire probability: {max_prob:.2%}")
                    if fire_count == 0:
                        print(f"  Result: ‚úÖ No fires detected")
                    else:
                        print(f"  Result: üî• Fire detected in {fire_count} frame(s)")
                    print(f"  ‚Üí Published to {self.video_completions_topic} for S3 upload")
                    print(f"{'='*60}\n")
                
                self.video_writer = None
                self.video_filepath = None
                self.video_frame_count = 0
                return filepath
            except Exception as e:
                print(f"Error closing video writer: {e}")
                import traceback
                traceback.print_exc()
                self.video_writer = None
                self.video_filepath = None
                return None
        return None
    
    def _reset_video_state(self, video_id: str, timestamp: str, frame_number: int):
        """Reset video state when video changes."""
        if video_id != self.current_video_id:
            # Close previous video writer if it exists
            if self.current_video_id is not None:
                self._close_video_writer(print_summary=True)
            
            # Reset for new video
            self.current_video_id = video_id
            self.video_fps = None
            self.video_width = None
            self.video_height = None
            self.video_start_timestamp = timestamp  # Store first frame timestamp
            self.last_frame_number = None
            self.last_frame = None
            
            # Initialize stats for new video
            if video_id not in self.video_stats:
                self.video_stats[video_id] = {"frames": 0, "fires": 0, "max_prob": 0.0}
    
    def _overlay_heatmap_on_frame(self, frame: np.ndarray, heatmap: Optional[np.ndarray], 
                                   alpha: float = None) -> np.ndarray:
        """
        Overlay heatmap on frame for visualization.
        
        Args:
            frame: Input frame (BGR format)
            heatmap: 2D heatmap array, or None
            alpha: Transparency of heatmap overlay (0.0-1.0), None to use config value
        
        Returns:
            Frame with heatmap overlaid
        """
        if heatmap is None:
            return frame
        
        if alpha is None:
            alpha = config.CLIP_HEATMAP_OVERLAY_ALPHA
        
        # Resize heatmap to match frame dimensions
        h, w = frame.shape[:2]
        heatmap_resized = cv2.resize(heatmap, (w, h))
        
        # Normalize heatmap to 0-255
        heatmap_norm = (heatmap_resized - heatmap_resized.min()) / (heatmap_resized.max() - heatmap_resized.min() + 1e-8)
        heatmap_uint8 = (heatmap_norm * 255).astype(np.uint8)
        
        # Apply colormap (JET for fire-like colors: blue->green->yellow->red)
        heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
        
        # Overlay heatmap on frame
        frame_with_heatmap = cv2.addWeighted(frame, 1.0 - alpha, heatmap_colored, alpha, 0)
        
        return frame_with_heatmap
    
    
    def process_frame(self, message_value: Dict[str, Any]) -> Dict[str, Any]:
        """Process a single frame through the ML model."""
        try:
            # Decode frame
            frame = self.decode_frame(message_value["frame_data"])
            
            # Get frame metadata
            video_id = message_value["video_id"]
            frame_number = int(message_value["frame_number"])  # Ensure it's an int
            timestamp = message_value["timestamp"]
            fps = message_value.get("fps", 30.0)
            width = message_value.get("width")
            height = message_value.get("height")
            
            # Check if video ended (large gap in frame numbers suggests video finished)
            # Note: Only close if gap is very large (>300 frames = ~10 seconds at 30fps)
            # This prevents premature closing due to processing delays or out-of-order frames
            if self.current_video_id == video_id and self.last_frame_number is not None:
                frame_gap = frame_number - self.last_frame_number
                # If gap is more than 300 frames (~10 seconds at 30fps), likely video ended
                # This is much larger than the previous 30-frame threshold to avoid false positives
                if frame_gap > 300:
                    print(f"‚ö†Ô∏è  Detected large gap of {frame_gap} frames - closing video {video_id}")
                    self._close_video_writer(print_summary=True)
                    # Reset state for new video (or continuation)
                    self._reset_video_state(video_id, timestamp, frame_number)
            
            # Reset state if video changed
            if video_id != self.current_video_id:
                self._reset_video_state(video_id, timestamp, frame_number)
            
            # Update last frame number
            self.last_frame_number = frame_number
            
            # Store video metadata and initialize writer if needed
            if self.video_fps is None:
                self.video_fps = fps
            if self.video_width is None and width:
                self.video_width = width
            if self.video_height is None and height:
                self.video_height = height
            
            # Initialize video writer if not already open (only once per video)
            if self.video_writer is None and self.video_width and self.video_height:
                self._initialize_video_writer(
                    video_id,
                    self.video_width, self.video_height, 
                    self.video_fps
                )
            
            # Run inference to get heatmap (for fire-detect-nn)
            prediction = self.model.predict(frame)
            
            # Update video statistics
            if video_id in self.video_stats:
                self.video_stats[video_id]["frames"] += 1
                if prediction["has_fire"]:
                    self.video_stats[video_id]["fires"] += 1
                    self.video_stats[video_id]["max_prob"] = max(
                        self.video_stats[video_id]["max_prob"],
                        prediction["fire_probability"]
                    )
            
            # Get heatmap if available
            heatmap = prediction.get("heatmap")
            
            # Overlay heatmap on frame if available
            processed_frame = frame.copy()
            if heatmap is not None:
                processed_frame = self._overlay_heatmap_on_frame(processed_frame, heatmap)
            
            # Write frame directly to video file (incremental writing)
            if self.video_writer is not None:
                self.video_writer.write(processed_frame)
                self.video_frame_count += 1
                # Store last frame for final buffer flush
                self.last_frame = processed_frame.copy()
                
                # Periodically check if writer is still working (every 100 frames)
                # This helps catch issues early and ensures frames are being written
                if self.video_frame_count % 100 == 0:
                    # Verify file is growing (basic sanity check)
                    if self.video_filepath and os.path.exists(self.video_filepath):
                        current_size = os.path.getsize(self.video_filepath)
                        # If file hasn't grown in a while, there might be an issue
                        # (but don't spam logs - just verify it's working)
                        pass
            
            # Create detection result
            detection_result = {
                "video_id": video_id,
                "frame_number": frame_number,
                "timestamp": timestamp,
                "processing_timestamp": datetime.utcnow().isoformat(),
                "has_fire": prediction["has_fire"],
                "fire_probability": prediction["fire_probability"],
                "detections": prediction["detections"],
                "frame_metadata": {
                    "width": message_value.get("width"),
                    "height": message_value.get("height"),
                    "fps": fps
                }
            }
            
            # Convert all numpy types to native Python types for JSON serialization
            detection_result = convert_numpy_types(detection_result)
            
            return detection_result
            
        except Exception as e:
            print(f"Error processing frame: {e}")
            return None
    
    def _cleanup(self):
        """Cleanup resources and finalize video."""
        print("\nüõë Cleaning up and finalizing video...")
        if self.video_writer is not None:
            try:
                self._close_video_writer(print_summary=True)
            except Exception as e:
                print(f"Error during cleanup: {e}")
    
    def run(self):
        """Run the stream processor."""
        print(f"Starting fire detection stream processor...")
        print(f"Consuming from topic: {config.KAFKA_VIDEO_TOPIC}")
        print(f"Publishing to topic: {config.KAFKA_DETECTIONS_TOPIC}")
        print(f"Waiting for messages... (Press Ctrl+C to stop)\n")
        
        # Register signal handlers for graceful shutdown
        def signal_handler(sig, frame):
            print(f"\n\nReceived signal {sig}, shutting down gracefully...")
            self._cleanup()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        message_count = 0
        detection_count = 0
        fire_count = 0
        
        try:
            for message in self.consumer:
                message_count += 1
                video_id = message.key
                frame_data = message.value
                
                if message_count % 10 == 0:  # Print every 10th frame to reduce noise
                    print(f"Processed {message_count} frames... (latest: frame {frame_data.get('frame_number')} from video {video_id})")
                
                # Process frame
                detection_result = self.process_frame(frame_data)
                
                if detection_result:
                    detection_count += 1
                    
                    # Only send if fire detected or for all frames (configurable)
                    if detection_result["has_fire"]:
                        fire_count += 1
                        print(f"üî• Fire detected! Frame {frame_data.get('frame_number')} from video {video_id} - Probability: {detection_result['fire_probability']:.2%}")
                    
                    # Send ALL detection results to topic (not just fires)
                    # This allows tracking of all processed frames
                    # Use async send (non-blocking) for better throughput
                    future = self.producer.send(
                        self.detections_topic,
                        key=video_id,
                        value=detection_result
                    )
                    
                    # Only check result for fire detections (to reduce overhead)
                    if detection_result["has_fire"]:
                        # Use callback for async error handling (non-blocking)
                        def on_send_success(record_metadata):
                            print(f"  ‚Üí Detection sent to topic {record_metadata.topic} partition {record_metadata.partition}")
                        
                        def on_send_error(excp):
                            print(f"  ‚ö†Ô∏è  Error sending detection: {excp}")
                        
                        future.add_callback(on_send_success)
                        future.add_errback(on_send_error)
                    
                    # Commit offset after processing each message (manual commit)
                    # This ensures we don't lose progress if the consumer stops
                    if message_count % 100 == 0:  # Commit every 100 messages to reduce overhead
                        try:
                            self.consumer.commit()
                        except Exception as e:
                            print(f"‚ö†Ô∏è  Error committing offset: {e}")
                else:
                    print(f"‚ö†Ô∏è  Failed to process frame {frame_data.get('frame_number')} from video {video_id}")
            
            # Final commit before closing
            try:
                self.consumer.commit()
            except:
                pass
            
            # If we exit the loop (timeout or end of messages), close current video
            if self.video_writer is not None:
                print(f"\n‚ö†Ô∏è  Consumer loop exited - processed {message_count} messages total")
                print("  Closing current video...")
                self._close_video_writer(print_summary=True)
        
        except KeyboardInterrupt:
            print(f"\n\nStopping stream processor...")
            self._cleanup()
            print(f"Summary: Processed {message_count} messages, {detection_count} detections, {fire_count} fires")
        except Exception as e:
            print(f"\nError in stream processor: {e}")
            self._cleanup()
            import traceback
            traceback.print_exc()
        finally:
            # Ensure cleanup happens even if something goes wrong
            if self.video_writer is not None:
                try:
                    self._close_video_writer(print_summary=False)
                except:
                    pass
            self.consumer.close()
            self.producer.flush()
            self.producer.close()
            print(f"Final summary: {message_count} messages processed, {detection_count} detections created, {fire_count} fires detected")


if __name__ == "__main__":
    stream = FireDetectionStream()
    stream.run()
